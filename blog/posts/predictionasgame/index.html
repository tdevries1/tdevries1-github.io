<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.4.1 by Michael Rose
  Copyright 2017 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE.txt
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->







<div class="fluid-row" id="header">




<title>Prediction as a game: review of past literature</title>




<meta name="description" content="!!!">




<meta name="author" content="Davide Viviano">

<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Davide Viviano">
<meta property="og:title" content="Prediction as a game, a quick review of past literature">


  <link rel="canonical" href="https://dviviano.github.io/posts/predictionasgame/">
  <meta property="og:url" content="https://dviviano.github.io/posts/predictionasgame/">



  <meta property="og:description" content="!!!">



  




  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2017-08-25T00:00:00-07:00">













<!-- end SEO -->

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://dviviano.github.io/assets/css/main.css">

<!--[if lte IE 9]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body class="layout--single">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <a class="site-title" href="https://dviviano.github.io/">Davide Viviano</a>
        <ul class="visible-links">
          
            
            <li class="masthead__menu-item"><a href="https://dviviano.github.io/">About</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://dviviano.github.io/resume.pdf">Resume</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://dviviano.github.io/presentations/">Presentations</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://dviviano.github.io/blog/">Personal Blog</a></li>
          
        </ul>
        <button><div class="navicon"></div></button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    



<div id="main" role="main">
  
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="https://dviviano.github.io/img/copertina2.jpg" class="author__avatar" alt="" itemprop="image">
      
    </div>
  

  <div class="author__content">
    <h3 class="author__name" itemprop="name">Davide Viviano</h3>
    
      <p class="author__bio" itemprop="description">
        PhD Candidate, UC San Diego.
      </p>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="http://schema.org/Place">
          <i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> <span itemprop="name">San Diego, CA</span>
        </li>
      

      

      

      

      
      
      

      

      

      

      
        <li>
          <a href="https://github.com/dviviano" itemprop="sameAs">
            <i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs">
      <i class="fa fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
    
    
  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Prediction as a game, a quick review of past literature">
    <meta itemprop="description" content="!!!">
    <meta itemprop="datePublished" content="June 25, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Prediction as a game: review of past literature
</h1>
          
        </header>
      

      <section class="page__content" itemprop="text">
        <p> In this article we provide a general understanding of sequential prediction, with a particular attention to adversarial models. The aim is to provide generical foundations to the problem and to discuss some of its well known application in Economics. The article is mainly based on few key references that the reader can find at the end of the blog article.</p>
<div id="sequential-prediction" class="section level2">
<h2>Sequential prediction</h2>
<p>Forecasting is a key issue in many circumstances, from weather prediction to predicting stock market returns. We discuss the following setting: Nature decides the outcome for a certain event(say, whether tomorrow it will rain or being sunny) , and the forecaster must make its guess before Nature reveals its choice. Once Nature reveals the outcome of its decision, the forecaster incurrs a loss.</p>
<p>More formally, the general set up considers <span class="math inline">\(y_1, y_2,... \in \mathcal{Y}\)</span> being the outcome space. The forecaster chooses an action <span class="math inline">\(I_1 , I_2,... \in \mathcal{D}\)</span> where <span class="math inline">\(\mathcal{D}\)</span> is the decision space and she incurres a loss at time <span class="math inline">\(t\)</span>, <span class="math inline">\(l(I_t, y_t)\)</span>. In an oblivious game the environment chooses the outcome regardless of the strategy of the opponent. Once Nature reveals its choice, the forecaster incurr a loss. We define <span class="math inline">\(\hat{L}_n = \sum_{t=1}^n l(I_t, y_t)\)</span> the cumulative loss of the forecaster. Some examples of loss functionsare the 0/1 loss - 0 when the forecast is correct, 1 otherwise, the exponential loss and many others. Without loss of generality we can impose that <span class="math inline">\(l(I_t,y_t) \in [0,1]\)</span>. The regret at time <span class="math inline">\(t\)</span> for choosing action <span class="math inline">\(I_t\)</span> instead of action <span class="math inline">\(i \in \{1,...,M\}\)</span> is defined as <span class="math inline">\(l(I_t,y_t) - l(i,y_t)\)</span>. Intuitevely, the regret indicates how much better your forecast was with respect to forecast of expert <span class="math inline">\(i\)</span>. A natural objective function is the average maximum regret faced by the forecaster. The maximum regret is defined as <span class="math display">\[
\frac{1}{n}\sum_{t=1}^n l(I_t, y_t) - min_{i=1,...,M}\frac{1}{n}\sum_{t=1}^nl(i,y_t)
\]</span> Forecasting strategies that guarantee that the average regret goes to zero almost surely for all possible strategies of the environment are defined <span class="math inline">\(Hannan\)</span> <span class="math inline">\(consistent\)</span>.</p>
</div>
<div id="prediction-with-experts-advice" class="section level2">
<h2>Prediction with experts advice</h2>
<p>Think about yourself choosing between organizing a trip to the beach or studying in the library. You must make your decision the day before the trip. You have access to <span class="math inline">\(M\)</span> weather prevision. Some of them tell you that it is going to rain and , therefore, it would be better to stay in the library studying and others tell you that there will be sun! How can you choose among so many advices? Is there any strategy that is at least consistent over time?</p>
<p>We start our discussion considering the context of regret minimization for prediction with experts advice [3] under a convex loss function. With an abuse of notation we consider the decision space being <span class="math inline">\(f_{i,1} , f_{i,2},... \in \mathcal{D}\)</span> of each expert <span class="math inline">\(i \in \{1,...,M\}\)</span>.</p>
<p><strong><em>Prediction protocol with convex loss function</em></strong></p>
<p>For each round <span class="math inline">\(t=1,2,...\)</span>:</p>
<ul>
<li>the environment chooses the next outcome <span class="math inline">\(y_t \in \{1,...,M\}\)</span> without reveling it;</li>
<li>each expert reveals its prediction to the forecaster;</li>
<li>the forecaster chooses its forecast being <span class="math inline">\(I_t\)</span>;</li>
<li>the environment reveals <span class="math inline">\(y_t\)</span>;</li>
<li>the forecaster suffers the loss <span class="math inline">\(l(I_t, y_t)\)</span> and each expert <span class="math inline">\(i\)</span> suffers a loss <span class="math inline">\(l(f_{i,t}, y_t)\)</span></li>
</ul>
<p>Consider the strategy consisting in doing a weighted average of experts’ advices. More formally: <span class="math display">\[
I_t =\sum_{i=1}^M \frac{W_{i,t}}{W_t}f_{i,t}
\]</span> with <span class="math inline">\(W_{i,t} = \exp(-\eta \sum_{s=1}^{t-1}l(f_{i,s}, y_s)) = \exp(-\eta L_{i,t-1})\)</span>, <span class="math inline">\(w_{i,0} = 1\)</span>, <span class="math inline">\(W_t = \sum_{i=1}^M W_{i,t}\)</span>, <span class="math inline">\(\eta &gt; 0\)</span>. The strategy is the so called exponential weighted forecast. Importantly, the strategy is consistent at a rate proportional to <span class="math inline">\(n^{-1/2}\)</span>, with <span class="math inline">\(n\)</span> being the number of rounds. This means that for <span class="math inline">\(n\)</span> being large enough, the strategy eventually converges to the optimal forecast. In the next lines we sketch the proof of this result from [2], [3].</p>
<p><strong>Sketch of the Proof.</strong> Considers log(<span class="math inline">\(W_N/W_0\)</span>) = log(<span class="math inline">\(W_N\)</span>) - log(<span class="math inline">\(M\)</span>) = <span class="math inline">\(\text{log}(\sum_{i=1}^M \exp(-\eta L_{i,n})) - \text{log}(M) = z\)</span> . Then a lower bound can be defined as <span class="math display">\[
z \ge \text{log}(max_{i=1,...,M} \exp(-\eta L_{i,n})) - \text{log}(M) = - \eta min_{i=1,...,M}L_{i,n} - \text{log}(M)
\]</span> Notice then that <span class="math inline">\(\text{log}(\frac{W_N}{W_0}) = \text{log}(\prod_{t=1}^N W_t/W_{t-1}) = \sum_{t=1}^N \text{log}(W_t/W_{t-1}) = \sum_{t=1}^N \text{log}(\sum_{i=1}^M q_{i,t-1} \exp(-\eta l(f_{i,t}, y_t))\)</span>, with <span class="math inline">\(q_{i,t-1} = \frac{w_{i,t-1}}{w_{t-1}}\)</span>. Given that the loss is bounded between <span class="math inline">\(0,1\)</span>, by Hoeffding inequality: <span class="math display">\[
\sum_{t=1}^N \text{log}(\sum_{i=1}^M q_{i,t-1} \exp(-\eta l(f_{i,t}, y_t)) \le -\eta \sum_{t=1}^N \frac{w_{i,t-1}}{W_{t-1}} l(f_{i,t}, y_t) + \frac{n \eta^2}{8} = c
\]</span> By Jensen’s inequality <span class="math display">\[
c \le -\eta l(\sum_{t=1}^N \frac{w_{i,t-1}}{W_{t-1}}f_{i,t}, y_t) +  \frac{n\eta^2}{8} = -\eta l(\hat{p}_t, y_t) +  \frac{n\eta^2}{8}
\]</span> Therefore we get <span class="math inline">\(-\eta \hat{L}_n + \frac{n\eta^2}{8} \ge -\eta min_{i=1,...,M} L_{i,n} - \text{log}(M)\)</span>. Rearrenging things we get <span class="math display">\[
\hat{L}_n - min_{i=1,...,M} L_{i,n} \le \frac{\text{log}(M)}{\eta} + \frac{n \eta}{8}
\]</span> By the first order conditions on the upper bound we choose <span class="math inline">\(\eta = \sqrt{\frac{8\text{log}(M)}{n}}\)</span> and by substituting the term we get: <span class="math display">\[
\hat{L}_n - min_{i=1,...,M} L_{i,N} \le \sqrt{\frac{n}{2} \text{log}(M)}
\]</span> Letting <span class="math inline">\(\eta_t\)</span> change over time and setting <span class="math inline">\(\eta_t = \sqrt{\frac{8\text{log}(M)}{t}}\)</span> you would get a bound of order <span class="math inline">\(O(\sqrt{n})\)</span>[2].</p>
<p>In the next line we outline a different strategy that has the same rate of consistency but it can be implemented with any loss function (also non convex).The main difference with the strategy described above is that this strategy radomizes the choice of the forecaster according to a vector of probabilities that we call <span class="math inline">\(\mathbf{p}_t\)</span>. The strategy is as follow.</p>
<p>For each round <span class="math inline">\(t=1,2,...\)</span>:</p>
<ul>
<li>the environment chooses the next outcome <span class="math inline">\(y_t \in \{1,...,M\}\)</span> without reveling it;</li>
<li>each expert reveals its prediction to the forecaster;</li>
<li>the forecaster chooses a probability vector <span class="math inline">\(\mathbf{p}_t\)</span> over the set of M actions and draws an action <span class="math inline">\(I_t \in \{1,...,M\}\)</span> with <span class="math display">\[
p_{i,t} = \frac{\exp(-\eta \sum_{s=1}^{t-1}l(f_{i,s}, y_s))}{W_{t-1}}f_{i,t}
\]</span></li>
<li>the environment reveals <span class="math inline">\(y_t\)</span>;</li>
<li>the forecaster suffers the loss <span class="math inline">\(l(I_t, y_t)\)</span> and each expert <span class="math inline">\(i\)</span> suffers a loss <span class="math inline">\(l(i, y_t)\)</span></li>
</ul>
<p>In this case the average regret is bounded by <span class="math inline">\(\sqrt{\frac{n}{2} \text{log}(\frac{1}{\delta})} + \sqrt{\frac{n}{2} \text{log}(M)}\)</span> with probability at least <span class="math inline">\(1 - \delta\)</span>.</p>
<p>Again, we sketch the proof from [3].</p>
<p><strong>Proof.</strong> To prove the result we first make use of the following lemma:</p>
<p><strong>Lemma 1.</strong> Let <span class="math inline">\(X_t \le 1\)</span> being a random variable such that <span class="math inline">\(E[X_t | \mathcal{F}_{t-1}]\)</span>, where <span class="math inline">\(\mathcal{F}_{t-1}\)</span> is the filtration at time <span class="math inline">\(t-1\)</span>. Then by Hoeffding-Azuma inequality <span class="math display">\[
P(\sum_{s=1}^n X_s - E[X_s] &gt; t) \le \exp(-2t^2/n) \Rightarrow \sum_{s=1}^n X_s - E[X_s] \le \sqrt{\frac{n}{2} \text{log}(1/\delta)} \quad \text{w.p.} \ge 1 - \delta 
\]</span></p>
<p>Define <span class="math inline">\(\bar{l}(\mathbf{p}_t, y_t) = \sum_{t=1}^N p_{i,t}l(i, y_t) = E[l(I_t, y_t)|\mathcal{F}_{t-1}]\)</span> where <span class="math inline">\(\mathcal{F}_{t-1}\)</span> is the filtration at time <span class="math inline">\(t-1\)</span>. Furthermore, notice that <span class="math inline">\(l(I_t, y_t)\)</span> is a martingale and <span class="math inline">\(\sum_{t=1}^n [l(I_t, y_t) - \bar{l}(\mathbf{p}_t, y_t)] = 0\)</span> has expectation <span class="math inline">\(0\)</span>. Using Hoeffding-Azuma<br />
<span class="math inline">\(\sum_{t=1}^n [l(I_t, y_t) - \bar{l}(\mathbf{p}_t, y_t)] \le \sqrt{\frac{n}{2}\text{log}(1/\delta)}\)</span> w.p. <span class="math inline">\(\ge 1 - \delta\)</span>, therefore the loss are concentrated around expectation. Notice now that <span class="math inline">\(\bar{l}(\mathbf{p}_t, y_t)\)</span> is convex (linear in this case) in the first variable. Therefore by the previous result <span class="math display">\[
\sum_{t=1}^N \bar{l}(\mathbf{p}_t, y_t) - min_{i=1,...,M} \sum_{t=1}^n l(i,y_t) \le \sqrt{\frac{n}{2} \text{log}(M)}
\]</span> By adding and subtracting <span class="math inline">\(\sum_{t=1}^N \bar{l}(\mathbf{p}_t, y_t)\)</span>: <span class="math display">\[
\sum_{t=1}^n l(I_t, y_t) - min_{i=1,...,M} \sum_{t=1}^n l(i,y_t) \le \sqrt{\frac{n}{2} \text{log}(1/\delta)} + \sqrt{\frac{n}{2} \text{log}(M)}
\]</span> which concludes the proof[3].</p>
</div>
<div id="multi-armed-bandit-problem" class="section level2">
<h2>Multi-Armed Bandit Problem</h2>
<p>We now move to a more complex scenario. After studying in the library on sunny days and going to the beach on raining days you give up in organizing trips. On the other hand, you still want to enjoy your life and you decide to go to restaurants every week. Unfortunately you have no idea of which restaurants are good and which are bad. Every time you eat in a restaurant you are able to assess the loss corresponding to this choice, but you do not know whether the other restaurants were better or worse. Again, you rely on asymptotics to hope that one day you will end up in a decent restaurant.</p>
<p>Consider the following prediction protocol:</p>
<p><strong>Prediction Protocol: Multi-Armed Bandit Problem</strong></p>
<p>For each round <span class="math inline">\(t=1,2,...\)</span></p>
<ul>
<li>the environment chooses the next outcome <span class="math inline">\(y_t \in \{1,...,M\}\)</span> without reveling it;</li>
<li>the forecaster chooses a probability vector <span class="math inline">\(\mathbf{p}_t\)</span> over the set of M actions and draws an action <span class="math inline">\(I_t \in \{1,...,M\}\)</span>;</li>
<li>the forecaster suffers the loss <span class="math inline">\(l(I_t, y_t)\)</span>;</li>
<li>only <span class="math inline">\(l(I_t, y_t)\)</span> is reveled to the forecaster, the loss for all other actions remain unknown.</li>
</ul>
<p>The objective function of the forecaster remains the regret. Clearly the situation is much more challenging, provided that there is not knowledge about the loss that would be incurred by taking a different choice. Auer et al. showed a Hannan consistent strategy at rate <span class="math inline">\(O(\sqrt{n})\)</span>. We define the following unbiased estimator: <span class="math display">\[
\tilde{l}(i, y_t) = \frac{l(i, y_t) \mathbf{1}_{I_t = i}}{p_{i,t}}
\]</span> where <span class="math inline">\(p_{i,t}\)</span> is the probability of choosing action <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span> and <span class="math inline">\(\mathbf{1}_x\)</span> is the indicator variable equal to one if <span class="math inline">\(x\)</span> is true, <span class="math inline">\(0\)</span> otherwise. Notice that <span class="math display">\[
E_t[\tilde{l}(i,  y_t)] = \sum_{j=1}^M p_{j,t} \frac{l(i, y_t) \mathbf{1}_{i = j}}{p_{i,t}} = l(i,y_t)
\]</span></p>
<p><strong>A forecasting strategy in Multi-Armed Bandit Problem</strong></p>
<p>We define <span class="math inline">\(g(i, y_t) = 1 - l(i, y_t)\)</span> the gain and <span class="math inline">\(\tilde{g}(i, y_t) = \frac{g(i, y_t)}{p_{i,t}}\mathbf{1}_{I_t = i}\)</span> the estimated unbiased gain. Notice that <span class="math inline">\(g(i, y_t) - \tilde{g}(i, y_t)\)</span> is at most 1, a property used for a martingale-type bound. Choose <span class="math inline">\(\eta, \gamma, \beta &gt; 0\)</span>. Initialize <span class="math inline">\(w_{i, 0} = 1, p_{i,1} = 1/M\)</span>.</p>
<p>For each round <span class="math inline">\(t = 1,2,...\)</span></p>
<ul>
<li>Select an action <span class="math inline">\(I_t\)</span> according to the probability distribution <span class="math inline">\(\mathbf{p}_t\)</span>;</li>
<li>calculate the estimated gain: <span class="math display">\[
g'(i, y_t) = \tilde{g}(i, y_t) + \beta/p_{i,t}
\]</span></li>
<li>update the weights <span class="math inline">\(w_{i,t} = w_{i,t-1}\exp(\eta g'(i,y_t))\)</span>;</li>
<li>update the probabilities <span class="math inline">\(p_{i,t+1} = (1 - \gamma)\frac{w_{i,t}}{W_t} + \frac{\gamma}{M}\)</span> with exponential weights.</li>
</ul>
<p>Note that by introducing a parameter <span class="math inline">\(\beta\)</span> we give up the unbiasedness of the estimate to guarantee that the estimated cumulative gains are, with large probability, not much smaller than the actual cumulative gains. Under conditions of theorem 6.10 [2] the regret is <span class="math inline">\(O(\sqrt{n})\)</span>, in particular <span class="math display">\[
\hat{L}_n - min_{i=1,...,M} L_{i,n} \le 11/2\sqrt{n M \text{log}(M/\delta)} + \text{log}(M)/2
\quad w.p. \ge 1 - \delta
\]</span> We want to stress that the main ingredients for an optimal rate of convergency in probability are contained in the so called “exploration-exploitation trade off”. In fact notice that <span class="math display">\[
p_{i,t} = \frac{\exp(-\eta \sum_{s=1}^{t-1} \tilde{l}(i, y_s))}{\sum_{j=1}^M \exp(-\eta \sum_{s=1}^{t-1} \tilde{l}(j, y_s))}(1 - \gamma) + \frac{\gamma}{M}
\]</span> then the first term multiplying by <span class="math inline">\(1 - \gamma\)</span> contains information regarding the losses of the actions taken in the past. The second term instead let the forecaster have non-zero probabilities for exploring new actions.</p>
</div>
<div id="partial-monitoring-multi-armed-bandit-problem" class="section level2">
<h2>Partial Monitoring Multi-Armed Bandit Problem</h2>
<p>We finish this article by describing a new interesting scenario. Consider a vendor selling a product to customers one by one. She can select a different price for each customer but no barganing is allowed and no further information can be exchanged between the buyer and the seller. Assume that the willingness to pay of each buyer is <span class="math inline">\(y_t\)</span> <span class="math inline">\(\in [0,1]\)</span>, the actual price offered to the seller is <span class="math inline">\(z_t\)</span> and the loss incurred by the seller at time <span class="math inline">\(t\)</span> is <span class="math display">\[
l(p_t, y_t) = (y_t - z_t) \mathbf{1}_{z_t \le y_t} + c\mathbf{1}_{z_t &gt; y_t}
\]</span> with <span class="math inline">\(c \in [0,1]\)</span>. The seller can only observe whether the customer buys or not the product and has no clue about the empirical distribution of <span class="math inline">\(y_t\)</span>. A natural question is whether it exists a randomized strategy for the seller such that the average regret is <span class="math inline">\(Hannan\)</span> <span class="math inline">\(consistent\)</span>.</p>
<p>In a more general setting we define the following prediction protocol:</p>
<p><strong><em>Prediction Protocol: Partial Monitoring Multi-Armed Bandit Problem</em></strong></p>
<p>For each round <span class="math inline">\(t=1,2,...\)</span></p>
<ul>
<li>the environment chooses the next outcome <span class="math inline">\(y_t \in \{1,...,M\}\)</span> without reveling it;</li>
<li>the forecaster chooses a probability vector <span class="math inline">\(\mathbf{p}_t\)</span> over the set of M actions and draws an action <span class="math inline">\(I_t \in \{1,...,M\}\)</span>;</li>
<li>the forecaster suffers the loss <span class="math inline">\(l(I_t, y_t)\)</span>;</li>
<li>only a feedback <span class="math inline">\(h(I_t, y_t)\)</span> is reveled to the forecaster.</li>
</ul>
<p>The losses of the forecaster can be summurized in the loss matrix <span class="math inline">\(\mathbf{L} = [l(i,j)]_{N \times M}\)</span>. With no loss of generality <span class="math inline">\(l(I_t, y_t) \in [0,1]\)</span>. At every iteration the forecaster chooses an action <span class="math inline">\(I_t\)</span>, suffers a loss <span class="math inline">\(l(I_t, y_t)\)</span> but she only observes a feedback <span class="math inline">\(h(I_t, y_t)\)</span> parametrized by a given feedback function <span class="math inline">\(h\)</span> that assigns to each action/outcome pair <span class="math inline">\(\in \{1,...,N\} \times \{1,...,M\}\)</span> an element of a finite set <span class="math inline">\(\mathcal{S} = \{s_1,...,s_m\}\)</span> of signals. The values are collected in the feedback matrix <span class="math inline">\(\mathbf{H} = [h(i,j)]_{N \times M}\)</span>. Notice that the forecaster at time <span class="math inline">\(t\)</span> has access only to the information <span class="math inline">\((h(I_1, yL = _1),..., h(I_{t-1}, y_{t-1}))\)</span>. In [1] the following strategy was shown to be <span class="math inline">\(Hannan\)</span> <span class="math inline">\(consistent\)</span> at a sub-optimal rate <span class="math inline">\(O(n^{-1/3})\)</span>. Assume that <span class="math inline">\(l(i,j) = \sum_{l=1}^N k(i,l) h(l,j)\)</span>, that is <span class="math inline">\(\mathbf{L} = \mathbf{K} \mathbf{H}\)</span>, considering <span class="math inline">\(\mathbf{H}\)</span> and <span class="math inline">\([\mathbf{H} \quad \mathbf{L}]\)</span> having the same rank. Define <span class="math inline">\(k^* = \text{max}_{i,j}\{1, |k(i,j)|\}\)</span> and as an unbiased estimator of the loss: <span class="math display">\[
\tilde{l}(i, y_t) = \frac{k(i, I_t) h(I_t, y_t)}{p_{I_t, t}} \quad \forall i = 1,...,N
\]</span> with <span class="math inline">\(p_{I_t, t}\)</span> being the probability of having chosen action <span class="math inline">\(I_t\)</span> at time <span class="math inline">\(t\)</span> and <span class="math inline">\(\tilde{L}_{i,t} = \sum_{s=1}^t \tilde{l}(i,y_t)\)</span>. Initialize <span class="math inline">\(\tilde{L}_{1,0} = ... = \tilde{L}_{M,0} = 0\)</span>.</p>
<p>For each round <span class="math inline">\(t = 1,2,...\)</span></p>
<ul>
<li>Let <span class="math inline">\(\eta_t = (k^*)^{-2/3}((ln M)/M)^{2/3} t^{-2/3}\)</span> and <span class="math inline">\(\gamma_t = (k^*)^{2/3}M^{2/3}(ln M)^{1/3}t^{-1/3}\)</span>;</li>
<li>choose an action <span class="math inline">\(I_t\)</span> from the set of actions <span class="math inline">\(\{1,...,M\}\)</span> at random accordding to the distribution <span class="math inline">\(\mathbf{p}_t\)</span> defined by <span class="math display">\[
p_{i,t} = (1 - \gamma_t) \frac{e^{-\eta_t \tilde{L}_{i,t-1}}}{\sum_{k=1}^M e^{-\eta_t \tilde{L}_{k,t-1}}} + \frac{\gamma_t}{M}
\]</span></li>
<li>let <span class="math inline">\(\tilde{L}_{i,t} = \tilde{L}_{i,t-1} + \tilde{l}(i, y_t)\)</span> for all $ i = 1,…,M$</li>
</ul>
<p>In [1] the authors showed that under some mild conditions the strategy has a performance bound with a magnitude proportional to <span class="math inline">\(n^{2/3}(k^* M)^{2/3}(ln M)^{1/3}\)</span>, that is with a convergency rate <span class="math inline">\(O(n^{-1/3})\)</span>.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li>[1] Cesa-Bianchi, Nicolo, Gábor Lugosi, and Gilles Stoltz. “Regret minimization under partial monitoring.” Mathematics of Operations Research 31.3 (2006): 562-580.</li>
<li>[2] Cesa-Bianchi, Nicolo, and Gábor Lugosi. Prediction, learning, and games. Cambridge university press, 2006.</li>
<li>[3] Cesa-Bianchi, Nicolo, and Gábor Lugosi. “On prediction of individual sequences.” The Annals of Statistics 27.6 (1999): 1865-1895.</li>
<li>[4] Helmbold, David P., Nicholas Littlestone, and Philip M. Long. “Apple tasting.” Information and Computation 161.2 (2000): 85-139.</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Updated:</strong> <time datetime="2017-06-25T00:00:00-07:00">June 25, 2017</time></p>
        
      </footer>

    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
      <li><a href="http://github.com/dviviano"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    

      
       </ul>
</div>

      </footer>
    </div>

    <script src="https://dviviano.github.io/assets/js/main.min.js"></script>




<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


  </body>
</html>
